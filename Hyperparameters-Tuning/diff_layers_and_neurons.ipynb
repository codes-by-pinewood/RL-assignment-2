{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "# Components of the agent: online_q_net, target_q_net, experience buffer\n",
        "\n",
        "class Replaybuffer:\n",
        "    def __init__(self, n_state, n_action):\n",
        "        self.n_state = n_state\n",
        "        self.n_action = n_action\n",
        "        self.size = 2000 # Memory pool size\n",
        "        self.batchsize = 10\n",
        "\n",
        "        # Allocate space for the experience tuple\n",
        "        self.s = np.empty(shape=(self.size, self.n_state), dtype=np.float32)\n",
        "        self.a = np.random.randint(low=0, high=n_action, size=self.size, dtype=np.uint8)\n",
        "        self.r = np.empty(self.size, dtype=np.float32)\n",
        "        self.done = np.random.randint(low=0, high=2, size=self.size, dtype=np.uint8)\n",
        "        self.s_ = np.empty(shape=(self.size, self.n_state), dtype=np.float32)\n",
        "\n",
        "        self.t = 0\n",
        "        self.tmax = 0  # Initialize tmax attribute\n",
        "\n",
        "    def add_memo(self, s, a, r, done, s_): # Functionality needed: 1. Add memory after interaction 2. Take out memory when sampling a batch\n",
        "    # Add memory to the memory pool at step t\n",
        "        self.s[self.t] = s\n",
        "        self.a[self.t] = a\n",
        "        self.r[self.t] = r\n",
        "        self.done[self.t] = done\n",
        "        self.s_[self.t] = s_\n",
        "        self.t = self.t + 1 if self.t + 1 < self.size else 1 # When t reaches 2001, start adding from 1 again\n",
        "        self.tmax = max(self.tmax, self.t + 1)\n",
        "\n",
        "\n",
        "    def sample(self):\n",
        "    # Sampling logic: If the buffer has more experiences than batchsize, then sample; if fewer, then take as many as there are\n",
        "        if self.tmax > self.batchsize:\n",
        "           k = self.batchsize  # If the number of samples in the buffer is greater than or equal to the batch size, use the batch size\n",
        "        else:\n",
        "           k = self.tmax  # Otherwise, use the actual number of samples in the buffer\n",
        "\n",
        "        idxes = random.sample(range(0, self.tmax), k)  # Sample with the determined k value\n",
        "\n",
        "        batch_s = []\n",
        "        batch_a = []\n",
        "        batch_r = []\n",
        "        batch_done = []\n",
        "        batch_s_ = []\n",
        "\n",
        "        for idx in idxes: # Sample 64 pieces of data\n",
        "            batch_s.append(self.s[idx])\n",
        "            batch_a.append(self.a[idx])\n",
        "            batch_r.append(self.r[idx])\n",
        "            batch_done.append(self.done[idx])\n",
        "            batch_s_.append(self.s_[idx])\n",
        "\n",
        "        # Convert numpy arrays to torch tensors\n",
        "        batch_s = torch.as_tensor(np.asarray(batch_s), dtype=torch.float32)\n",
        "        batch_a = torch.as_tensor(np.asarray(batch_a), dtype=torch.int64).unsqueeze(-1) # Dimension increase: from (2) to (2,1)\n",
        "        batch_r = torch.as_tensor(np.asarray(batch_r), dtype=torch.float32).unsqueeze(-1)\n",
        "        batch_done = torch.as_tensor(np.asarray(batch_done), dtype=torch.float32).unsqueeze(-1)\n",
        "        batch_s_ = torch.as_tensor(np.asarray(batch_s_), dtype=torch.float32)\n",
        "\n",
        "        return batch_s, batch_a, batch_r, batch_done, batch_s_\n",
        "\n",
        "class Qnetwork(nn.Module):\n",
        "      def __init__(self, n_input, n_output):\n",
        "          super().__init__() #Inherit from the Module superclass\n",
        "\n",
        "          self.net = nn.Sequential(\n",
        "              nn.Linear(in_features= n_input, out_features = 128),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(in_features= 128, out_features = n_output))\n",
        "\n",
        "      def forward(self,x):\n",
        "           return self.net(x) #forward propagation\n",
        "\n",
        "      def act(self,obs): #In the face of s, find the maximum Q value (because the neural network outputs more than the maximum Q value) and output the corresponding action\n",
        "          obs_tensor = torch.as_tensor(obs, dtype=torch.float32)\n",
        "          q_value = self(obs_tensor.unsqueeze(0)) #convert it to the row vector\n",
        "          max_q_idx = torch.argmax(input=q_value)\n",
        "          action = max_q_idx.detach().item() #the action correspoding to the index of the highest Q-value\n",
        "          return action\n",
        "\n",
        "\n",
        "\n",
        "class Qnetwork2(nn.Module):\n",
        "      def __init__(self, n_input, n_output):\n",
        "          super().__init__()\n",
        "\n",
        "          self.net = nn.Sequential(\n",
        "              nn.Linear(in_features= n_input, out_features = 128),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(in_features= 128, out_features = 128),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(in_features= 128, out_features = n_output))\n",
        "\n",
        "      def forward(self,x):\n",
        "           return self.net(x)\n",
        "\n",
        "      def act(self,obs):\n",
        "          obs_tensor = torch.as_tensor(obs, dtype=torch.float32)\n",
        "          q_value = self(obs_tensor.unsqueeze(0))\n",
        "          max_q_idx = torch.argmax(input=q_value)\n",
        "          action = max_q_idx.detach().item()\n",
        "          return action\n",
        "\n",
        "class Qnetwork3(nn.Module):\n",
        "      def __init__(self, n_input, n_output):\n",
        "          super().__init__()\n",
        "\n",
        "          self.net = nn.Sequential(\n",
        "              nn.Linear(in_features= n_input, out_features = 32),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(in_features= 32, out_features = n_output))\n",
        "\n",
        "      def forward(self,x):\n",
        "           return self.net(x)\n",
        "\n",
        "      def act(self,obs):\n",
        "          obs_tensor = torch.as_tensor(obs, dtype=torch.float32)\n",
        "          q_value = self(obs_tensor.unsqueeze(0))\n",
        "          max_q_idx = torch.argmax(input=q_value)\n",
        "          action = max_q_idx.detach().item()\n",
        "          return action\n",
        "\n",
        "\n",
        "class Qnetwork4(nn.Module):\n",
        "      def __init__(self, n_input, n_output):\n",
        "          super().__init__()\n",
        "\n",
        "          self.net = nn.Sequential(\n",
        "              nn.Linear(in_features= n_input, out_features = 32),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(in_features= 32, out_features = 32),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(in_features= 32, out_features = n_output))\n",
        "\n",
        "      def forward(self,x):\n",
        "           return self.net(x)\n",
        "\n",
        "      def act(self,obs):\n",
        "          obs_tensor = torch.as_tensor(obs, dtype=torch.float32)\n",
        "          q_value = self(obs_tensor.unsqueeze(0))\n",
        "          max_q_idx = torch.argmax(input=q_value)\n",
        "          action = max_q_idx.detach().item()\n",
        "          return action\n",
        "\n",
        "\n",
        "\n",
        "class Qnetwork5(nn.Module):\n",
        "      def __init__(self, n_input, n_output):\n",
        "          super().__init__()\n",
        "\n",
        "          self.net = nn.Sequential(\n",
        "              nn.Linear(in_features= n_input, out_features = 128),\n",
        "              nn.ReLU(), #nn.Tanh(),\n",
        "              nn.Linear(in_features= 128, out_features = 128),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(in_features= 128, out_features = 128),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(in_features= 128, out_features = n_output))\n",
        "\n",
        "      def forward(self,x):\n",
        "           return self.net(x)\n",
        "\n",
        "      def act(self,obs):\n",
        "          obs_tensor = torch.as_tensor(obs, dtype=torch.float32)\n",
        "          q_value = self(obs_tensor.unsqueeze(0))\n",
        "          max_q_idx = torch.argmax(input=q_value)\n",
        "          action = max_q_idx.detach().item()\n",
        "          return action\n",
        "\n",
        "\n",
        "class Qnetwork6(nn.Module):\n",
        "      def __init__(self, n_input, n_output):\n",
        "          super().__init__()\n",
        "\n",
        "          self.net = nn.Sequential(\n",
        "              nn.Linear(in_features= n_input, out_features = 256),\n",
        "              nn.ReLU(), #nn.Tanh(),\n",
        "              nn.Linear(in_features= 256, out_features = n_output))\n",
        "\n",
        "      def forward(self,x):\n",
        "           return self.net(x)\n",
        "\n",
        "      def act(self,obs):\n",
        "          obs_tensor = torch.as_tensor(obs, dtype=torch.float32)\n",
        "          q_value = self(obs_tensor.unsqueeze(0))\n",
        "          max_q_idx = torch.argmax(input=q_value)\n",
        "          action = max_q_idx.detach().item()\n",
        "          return action\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Agent:\n",
        "   def __init__(self, n_input, n_output):\n",
        "            self.n_input = n_input\n",
        "            self.n_output = n_output\n",
        "\n",
        "            self.Gamma = 0.97\n",
        "            self.learning_rate = 0.01\n",
        "\n",
        "            self.memo = Replaybuffer(self.n_input, self.n_output)\n",
        "\n",
        "            #The two networks have the same structure, so it can be instantiated by a same class\n",
        "            self.online_net = Qnetwork(self.n_input, self.n_output)\n",
        "            self.target_net = Qnetwork(self.n_input, self.n_output)\n",
        "\n",
        "            self.optimizer = torch.optim.Adam(self.online_net.parameters(),\n",
        "                                              lr=self.learning_rate)\n",
        "\n",
        "class Agent2:\n",
        "   def __init__(self, n_input, n_output):\n",
        "            self.n_input = n_input\n",
        "            self.n_output = n_output\n",
        "\n",
        "            self.Gamma = 0.97\n",
        "            self.learning_rate = 0.001\n",
        "\n",
        "            self.memo = Replaybuffer(self.n_input, self.n_output)\n",
        "\n",
        "            self.online_net = Qnetwork(self.n_input, self.n_output)\n",
        "            self.target_net = Qnetwork(self.n_input, self.n_output)\n",
        "\n",
        "            self.optimizer = torch.optim.Adam(self.online_net.parameters(),\n",
        "                                              lr=self.learning_rate)\n",
        "\n",
        "class Agent3:\n",
        "   def __init__(self, n_input, n_output):\n",
        "            self.n_input = n_input\n",
        "            self.n_output = n_output\n",
        "\n",
        "            self.Gamma = 0.97\n",
        "            self.learning_rate = 0.05\n",
        "\n",
        "            self.memo = Replaybuffer(self.n_input, self.n_output)\n",
        "\n",
        "            self.online_net = Qnetwork(self.n_input, self.n_output)\n",
        "            self.target_net = Qnetwork(self.n_input, self.n_output)\n",
        "\n",
        "            self.optimizer = torch.optim.Adam(self.online_net.parameters(),\n",
        "                                              lr=self.learning_rate)\n",
        "\n",
        "\n",
        "#Different NN\n",
        "class Agent4:\n",
        "   def __init__(self, n_input, n_output):\n",
        "            self.n_input = n_input\n",
        "            self.n_output = n_output\n",
        "\n",
        "            self.Gamma = 0.97\n",
        "            self.learning_rate = 0.01\n",
        "\n",
        "            self.memo = Replaybuffer(self.n_input, self.n_output)\n",
        "\n",
        "            self.online_net = Qnetwork2(self.n_input, self.n_output)\n",
        "            self.target_net = Qnetwork2(self.n_input, self.n_output)\n",
        "\n",
        "            self.optimizer = torch.optim.Adam(self.online_net.parameters(),\n",
        "                                              lr=self.learning_rate)\n",
        "\n",
        "class Agent5:\n",
        "   def __init__(self, n_input, n_output):\n",
        "            self.n_input = n_input\n",
        "            self.n_output = n_output\n",
        "\n",
        "            self.Gamma = 0.97\n",
        "            self.learning_rate = 0.01\n",
        "\n",
        "            self.memo = Replaybuffer(self.n_input, self.n_output)\n",
        "\n",
        "            self.online_net = Qnetwork3(self.n_input, self.n_output)\n",
        "            self.target_net = Qnetwork3(self.n_input, self.n_output)\n",
        "\n",
        "            self.optimizer = torch.optim.Adam(self.online_net.parameters(),\n",
        "                                              lr=self.learning_rate)\n",
        "\n",
        "\n",
        "class Agent6:\n",
        "   def __init__(self, n_input, n_output):\n",
        "            self.n_input = n_input\n",
        "            self.n_output = n_output\n",
        "\n",
        "            self.Gamma = 0.97\n",
        "            self.learning_rate = 0.01\n",
        "\n",
        "            self.memo = Replaybuffer(self.n_input, self.n_output)\n",
        "\n",
        "            self.online_net = Qnetwork4(self.n_input, self.n_output)\n",
        "            self.target_net = Qnetwork4(self.n_input, self.n_output)\n",
        "\n",
        "            self.optimizer = torch.optim.Adam(self.online_net.parameters(),\n",
        "                                              lr=self.learning_rate)\n",
        "\n",
        "class Agent7:\n",
        "   def __init__(self, n_input, n_output):\n",
        "            self.n_input = n_input\n",
        "            self.n_output = n_output\n",
        "\n",
        "            self.Gamma = 0.97\n",
        "            self.learning_rate = 0.01\n",
        "\n",
        "            self.memo = Replaybuffer(self.n_input, self.n_output)\n",
        "\n",
        "            self.online_net = Qnetwork5(self.n_input, self.n_output)\n",
        "            self.target_net = Qnetwork5(self.n_input, self.n_output)\n",
        "\n",
        "            self.optimizer = torch.optim.Adam(self.online_net.parameters(),\n",
        "                                              lr=self.learning_rate)\n",
        "\n",
        "\n",
        "class Agent8:\n",
        "   def __init__(self, n_input, n_output):\n",
        "            self.n_input = n_input\n",
        "            self.n_output = n_output\n",
        "\n",
        "            self.Gamma = 0.97\n",
        "            self.learning_rate = 0.01\n",
        "\n",
        "            self.memo = Replaybuffer(self.n_input, self.n_output)\n",
        "\n",
        "            self.online_net = Qnetwork6(self.n_input, self.n_output)\n",
        "            self.target_net = Qnetwork6(self.n_input, self.n_output)\n",
        "\n",
        "            self.optimizer = torch.optim.Adam(self.online_net.parameters(),\n",
        "                                              lr=self.learning_rate)\n"
      ],
      "metadata": {
        "id": "y4jPT1YojAyz"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn #Classes and methods required for neural networks\n",
        "#from agent import Agent, Agent2, Agent3, Agent4, Agent5, Agent6\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env_name = \"CartPole-v1\"\n",
        "env = gym.make(env_name, render_mode='human')\n",
        "s = env.reset()\n",
        "\n",
        "EPSILON_DECAY = 10000\n",
        "EPSILON_START = 0.9\n",
        "EPSILON_END = 0.1\n",
        "TARGET_UPDATE = 5\n",
        "\n",
        "num_state = len(s)\n",
        "num_action = env.action_space.n #2\n",
        "\n",
        "n_episode = 1000\n",
        "n_step = 500"
      ],
      "metadata": {
        "id": "rQueUlEaPIpf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "373c2aaf-8787-4b49-af7d-5928b90e2d72"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Reward_list = np.empty(shape=n_episode)\n",
        "\n",
        "agent = Agent(n_input=num_state, n_output=num_action) #实例化\n",
        "episode_list = []\n",
        "reward_list = []\n",
        "\n",
        "for episode in range(n_episode):\n",
        "    epi_reward = 0\n",
        "    for step in range(n_step):\n",
        "        'epsilon greedy with decay of epsilon'\n",
        "        epsilon = np.interp(episode * n_step + step, [0, EPSILON_DECAY],\n",
        "                            [EPSILON_START, EPSILON_END]) # Interpolation\n",
        "#episode * n_step + step: The point to interpolate in the first step of the current episode\n",
        "#[0, EPSILON_DECAY] : data point horizontal coordinate [EPSILON_START, EPSILON_END] : data point vertical coordinate\n",
        "#epsilon value changes linearly from EPSILON_START to EPSILON_END within EPSILON_DECAY\n",
        "\n",
        "        random_sample = random.random()\n",
        "        if random_sample <= epsilon:\n",
        "           a = env.action_space.sample()\n",
        "        else:\n",
        "           a = agent.online_net.act(s) #todo\n",
        "        'Interact with the env'\n",
        "        #print(env.step(a))\n",
        "        s_, r, done , _ = env.step(a) #Perform action a，we get s_,r,done,info\n",
        "        agent.memo.add_memo(s, a, r, done, s_) #Add experiences into the exp buffer\n",
        "        s = s_ #store transition\n",
        "        epi_reward += r\n",
        "        #print(epi_reward)\n",
        "\n",
        "        if done:\n",
        "           s = env.reset()\n",
        "           Reward_list[episode] = epi_reward #Record the total reward of this episode\n",
        "           break\n",
        "\n",
        "        '''Sample minibatches from the transition'''\n",
        "        batch_s, batch_a, batch_r, batch_done, batch_s_ = agent.memo.sample()\n",
        "\n",
        "        '''Compute Q_target'''\n",
        "        target_q_values = agent.target_net(batch_s_)\n",
        "        target_q = batch_r + agent.Gamma * (1-batch_done) * target_q_values.max(dim=1, keepdim=True)[0]\n",
        "        '''Compute Q_pred'''\n",
        "        pred_q_values = agent.online_net(batch_s) # For each state in the batch, it gives the Q value for each action\n",
        "        pred_q = torch.gather(input=pred_q_values, dim=1, index=batch_a)\n",
        "       # Based on the action index specified in batch_a, select the corresponding action from the action Q value pred_q_values of each state\n",
        "        '''Compute Loss, gredient descent'''\n",
        "        loss = nn.functional.smooth_l1_loss(target_q, pred_q)\n",
        "        agent.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        agent.optimizer.step() # Descend according to the gradient\n",
        "\n",
        "        '''Fix Q-target'''\n",
        "    if episode % TARGET_UPDATE ==0:\n",
        "        agent.target_net.load_state_dict(agent.online_net.state_dict())\n",
        "        reward = np.mean(Reward_list[episode-10:episode])\n",
        "        print(\"Episode:{}\".format(episode))\n",
        "        print(\"Reward:{}\".format(reward))\n",
        "        episode_list.append(episode)\n",
        "        reward_list.append(reward)"
      ],
      "metadata": {
        "id": "w_U3Zsk5fy_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tuning Learning rates\n",
        "Reward_list2 = np.empty(shape=n_episode)\n",
        "agent2 = Agent2(n_input=num_state, n_output=num_action)\n",
        "episode_list2 = []\n",
        "reward_list2 = []\n",
        "\n",
        "for episode in range(n_episode):\n",
        "    epi_reward = 0\n",
        "    for step in range(n_step):\n",
        "        'epsilon greedy with decay of epsilon'\n",
        "        epsilon = np.interp(episode * n_step + step, [0, EPSILON_DECAY],\n",
        "                            [EPSILON_START, EPSILON_END])\n",
        "        random_sample = random.random()\n",
        "        if random_sample <= epsilon:\n",
        "           a = env.action_space.sample()\n",
        "        else:\n",
        "           a = agent2.online_net.act(s)\n",
        "        'Interact with the env'\n",
        "        #print(env.step(a))\n",
        "        s_, r, done, _ = env.step(a)\n",
        "        agent2.memo.add_memo(s, a, r, done, s_)\n",
        "        s = s_ #store transition\n",
        "        epi_reward += r\n",
        "        #print(epi_reward)\n",
        "\n",
        "        if done:\n",
        "           s = env.reset()\n",
        "           Reward_list2[episode] = epi_reward\n",
        "           break\n",
        "\n",
        "        '''Sample minibatches from the transition'''\n",
        "        batch_s, batch_a, batch_r, batch_done, batch_s_ = agent2.memo.sample()\n",
        "\n",
        "        '''Compute Q_target'''\n",
        "        target_q_values = agent2.target_net(batch_s_)\n",
        "        target_q = batch_r + agent2.Gamma * (1-batch_done) * target_q_values.max(dim=1, keepdim=True)[0]\n",
        "        '''Compute Q_pred'''\n",
        "        pred_q_values = agent2.online_net(batch_s)\n",
        "        pred_q = torch.gather(input=pred_q_values, dim=1, index=batch_a)\n",
        "\n",
        "        '''Compute Loss, gredient descent'''\n",
        "        loss = nn.functional.smooth_l1_loss(target_q, pred_q)\n",
        "        agent2.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        agent2.optimizer.step()\n",
        "\n",
        "        '''Fix Q-target'''\n",
        "    if episode % TARGET_UPDATE ==0:\n",
        "        agent2.target_net.load_state_dict(agent2.online_net.state_dict())\n",
        "        reward = np.mean(Reward_list2[episode-10:episode])\n",
        "        print(\"Episode:{}\".format(episode))\n",
        "        print(\"Reward:{}\".format(reward))\n",
        "        episode_list2.append(episode)\n",
        "        reward_list2.append(reward)\n",
        "\n",
        "\n",
        "\n",
        "Reward_list3 = np.empty(shape=n_episode)\n",
        "agent3 = Agent3(n_input=num_state, n_output=num_action)\n",
        "episode_list3 = []\n",
        "reward_list3 = []\n",
        "\n",
        "for episode in range(n_episode):\n",
        "    epi_reward = 0\n",
        "    for step in range(n_step):\n",
        "        'epsilon greedy with decay of epsilon'\n",
        "        epsilon = np.interp(episode * n_step + step, [0, EPSILON_DECAY],\n",
        "                            [EPSILON_START, EPSILON_END])\n",
        "\n",
        "        random_sample = random.random()\n",
        "        if random_sample <= epsilon:\n",
        "           a = env.action_space.sample()\n",
        "        else:\n",
        "           a = agent3.online_net.act(s)\n",
        "        'Interact with the env'\n",
        "        #print(env.step(a))\n",
        "        s_, r, done , _ = env.step(a)\n",
        "        agent3.memo.add_memo(s, a, r, done, s_)\n",
        "        s = s_ #store transition\n",
        "        epi_reward += r\n",
        "        #print(epi_reward)\n",
        "\n",
        "        if done:\n",
        "           s = env.reset()\n",
        "           Reward_list3[episode] = epi_reward\n",
        "           break\n",
        "\n",
        "        '''Sample minibatches from the transition'''\n",
        "        batch_s, batch_a, batch_r, batch_done, batch_s_ = agent3.memo.sample()\n",
        "\n",
        "        '''Compute Q_target'''\n",
        "        target_q_values = agent3.target_net(batch_s_)\n",
        "        target_q = batch_r + agent3.Gamma * (1-batch_done) * target_q_values.max(dim=1, keepdim=True)[0]\n",
        "        '''Compute Q_pred'''\n",
        "        pred_q_values = agent3.online_net(batch_s)\n",
        "        pred_q = torch.gather(input=pred_q_values, dim=1, index=batch_a)\n",
        "\n",
        "        '''Compute Loss, gredient descent'''\n",
        "        loss = nn.functional.smooth_l1_loss(target_q, pred_q)\n",
        "        agent3.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        agent3.optimizer.step()\n",
        "\n",
        "        '''Fix Q-target'''\n",
        "    if episode % TARGET_UPDATE ==0:\n",
        "        agent3.target_net.load_state_dict(agent3.online_net.state_dict())\n",
        "        reward = np.mean(Reward_list3[episode-10:episode])\n",
        "        print(\"Episode:{}\".format(episode))\n",
        "        print(\"Reward:{}\".format(reward))\n",
        "        episode_list3.append(episode)\n",
        "        reward_list3.append(reward)"
      ],
      "metadata": {
        "id": "1O4dn1jFvif-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tuning the network\n",
        "Reward_list4 = np.empty(shape=n_episode)\n",
        "agent4 = Agent4(n_input=num_state, n_output=num_action)\n",
        "episode_list4 = []\n",
        "reward_list4 = []\n",
        "\n",
        "for episode in range(n_episode):\n",
        "    epi_reward = 0\n",
        "    for step in range(n_step):\n",
        "        'epsilon greedy with decay of epsilon'\n",
        "        epsilon = np.interp(episode * n_step + step, [0, EPSILON_DECAY],\n",
        "                            [EPSILON_START, EPSILON_END])\n",
        "\n",
        "        random_sample = random.random()\n",
        "        if random_sample <= epsilon:\n",
        "           a = env.action_space.sample()\n",
        "        else:\n",
        "           a = agent4.online_net.act(s)\n",
        "        'Interact with the env'\n",
        "        #print(env.step(a))\n",
        "        s_, r, done , _ = env.step(a)\n",
        "        agent4.memo.add_memo(s, a, r, done, s_)\n",
        "        s = s_ #store transition\n",
        "        epi_reward += r\n",
        "        #print(epi_reward)\n",
        "\n",
        "        if done:\n",
        "           s = env.reset()\n",
        "           Reward_list4[episode] = epi_reward\n",
        "           break\n",
        "\n",
        "        '''Sample minibatches from the transition'''\n",
        "        batch_s, batch_a, batch_r, batch_done, batch_s_ = agent4.memo.sample()\n",
        "\n",
        "        '''Compute Q_target'''\n",
        "        target_q_values = agent4.target_net(batch_s_)\n",
        "        target_q = batch_r + agent4.Gamma * (1-batch_done) * target_q_values.max(dim=1, keepdim=True)[0]\n",
        "        '''Compute Q_pred'''\n",
        "        pred_q_values = agent4.online_net(batch_s)\n",
        "        pred_q = torch.gather(input=pred_q_values, dim=1, index=batch_a)\n",
        "\n",
        "        '''Compute Loss, gredient descent'''\n",
        "        loss = nn.functional.smooth_l1_loss(target_q, pred_q)\n",
        "        agent4.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        agent4.optimizer.step()\n",
        "\n",
        "        '''Fix Q-target'''\n",
        "    if episode % TARGET_UPDATE ==0:\n",
        "        agent4.target_net.load_state_dict(agent4.online_net.state_dict())\n",
        "        reward = np.mean(Reward_list4[episode-10:episode])\n",
        "        print(\"Episode:{}\".format(episode))\n",
        "        print(\"Reward:{}\".format(reward))\n",
        "        episode_list4.append(episode)\n",
        "        reward_list4.append(reward)\n",
        "\n",
        "\n",
        "\n",
        "Reward_list5 = np.empty(shape=n_episode)\n",
        "agent5 = Agent5(n_input=num_state, n_output=num_action)\n",
        "episode_list5 = []\n",
        "reward_list5 = []\n",
        "\n",
        "for episode in range(n_episode):\n",
        "    epi_reward = 0\n",
        "    for step in range(n_step):\n",
        "        'epsilon greedy with decay of epsilon'\n",
        "        epsilon = np.interp(episode * n_step + step, [0, EPSILON_DECAY],\n",
        "                            [EPSILON_START, EPSILON_END])\n",
        "        random_sample = random.random()\n",
        "        if random_sample <= epsilon:\n",
        "           a = env.action_space.sample()\n",
        "        else:\n",
        "           a = agent5.online_net.act(s)\n",
        "        'Interact with the env'\n",
        "        #print(env.step(a))\n",
        "        s_, r, done , _ = env.step(a)\n",
        "        agent5.memo.add_memo(s, a, r, done, s_)\n",
        "        s = s_ #store transition\n",
        "        epi_reward += r\n",
        "        #print(epi_reward)\n",
        "\n",
        "        if done:\n",
        "           s = env.reset()\n",
        "           Reward_list5[episode] = epi_reward\n",
        "           break\n",
        "\n",
        "        '''Sample minibatches from the transition'''\n",
        "        batch_s, batch_a, batch_r, batch_done, batch_s_ = agent5.memo.sample()\n",
        "\n",
        "        '''Compute Q_target'''\n",
        "        target_q_values = agent5.target_net(batch_s_)\n",
        "        target_q = batch_r + agent5.Gamma * (1-batch_done) * target_q_values.max(dim=1, keepdim=True)[0]\n",
        "        '''Compute Q_pred'''\n",
        "        pred_q_values = agent5.online_net(batch_s)\n",
        "        pred_q = torch.gather(input=pred_q_values, dim=1, index=batch_a)\n",
        "\n",
        "        '''Compute Loss, gredient descent'''\n",
        "        loss = nn.functional.smooth_l1_loss(target_q, pred_q)\n",
        "        agent5.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        agent5.optimizer.step()\n",
        "\n",
        "        '''Fix Q-target'''\n",
        "    if episode % TARGET_UPDATE ==0:\n",
        "        agent5.target_net.load_state_dict(agent5.online_net.state_dict())\n",
        "        reward = np.mean(Reward_list5[episode-10:episode])\n",
        "        print(\"Episode:{}\".format(episode))\n",
        "        print(\"Reward:{}\".format(reward))\n",
        "        episode_list5.append(episode)\n",
        "        reward_list5.append(reward)\n"
      ],
      "metadata": {
        "id": "LdsKh619fpwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Reward_list7 = np.empty(shape=n_episode)\n",
        "agent7 = Agent7(n_input=num_state, n_output=num_action)\n",
        "episode_list7 = []\n",
        "reward_list7 = []\n",
        "\n",
        "for episode in range(n_episode):\n",
        "    epi_reward = 0\n",
        "    for step in range(n_step):\n",
        "        'epsilon greedy with decay of epsilon'\n",
        "        epsilon = np.interp(episode * n_step + step, [0, EPSILON_DECAY],\n",
        "                            [EPSILON_START, EPSILON_END])\n",
        "\n",
        "        random_sample = random.random()\n",
        "        if random_sample <= epsilon:\n",
        "           a = env.action_space.sample()\n",
        "        else:\n",
        "           a = agent7.online_net.act(s)\n",
        "        'Interact with the env'\n",
        "        #print(env.step(a))\n",
        "        s_, r, done , _ = env.step(a)\n",
        "        agent7.memo.add_memo(s, a, r, done, s_)\n",
        "        s = s_ #store transition\n",
        "        epi_reward += r\n",
        "        #print(epi_reward)\n",
        "\n",
        "        if done:\n",
        "           s = env.reset()\n",
        "           Reward_list7[episode] = epi_reward\n",
        "           break\n",
        "\n",
        "        '''Sample minibatches from the transition'''\n",
        "        batch_s, batch_a, batch_r, batch_done, batch_s_ = agent7.memo.sample()\n",
        "\n",
        "        '''Compute Q_target'''\n",
        "        target_q_values = agent7.target_net(batch_s_)\n",
        "        target_q = batch_r + agent7.Gamma * (1-batch_done) * target_q_values.max(dim=1, keepdim=True)[0]\n",
        "        '''Compute Q_pred'''\n",
        "        pred_q_values = agent7.online_net(batch_s)\n",
        "        pred_q = torch.gather(input=pred_q_values, dim=1, index=batch_a)\n",
        "\n",
        "        '''Compute Loss, gredient descent'''\n",
        "        loss = nn.functional.smooth_l1_loss(target_q, pred_q)\n",
        "        agent7.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        agent7.optimizer.step()\n",
        "\n",
        "        '''Fix Q-target'''\n",
        "    if episode % TARGET_UPDATE ==0:\n",
        "        agent7.target_net.load_state_dict(agent7.online_net.state_dict())\n",
        "        reward = np.mean(Reward_list7[episode-10:episode])\n",
        "        print(\"Episode:{}\".format(episode))\n",
        "        print(\"Reward:{}\".format(reward))\n",
        "        episode_list7.append(episode)\n",
        "        reward_list7.append(reward)"
      ],
      "metadata": {
        "id": "5IqraWxPCwHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Reward_list8 = np.empty(shape=n_episode)\n",
        "agent8 = Agent8(n_input=num_state, n_output=num_action)\n",
        "episode_list8 = []\n",
        "reward_list8 = []\n",
        "\n",
        "for episode in range(n_episode):\n",
        "    epi_reward = 0\n",
        "    for step in range(n_step):\n",
        "        'epsilon greedy with decay of epsilon'\n",
        "        epsilon = np.interp(episode * n_step + step, [0, EPSILON_DECAY],\n",
        "                            [EPSILON_START, EPSILON_END])\n",
        "\n",
        "        random_sample = random.random()\n",
        "        if random_sample <= epsilon:\n",
        "           a = env.action_space.sample()\n",
        "        else:\n",
        "           a = agent8.online_net.act(s)\n",
        "        'Interact with the env'\n",
        "        #print(env.step(a))\n",
        "        s_, r, done , _ = env.step(a)\n",
        "        agent8.memo.add_memo(s, a, r, done, s_)\n",
        "        s = s_ #store transition\n",
        "        epi_reward += r\n",
        "        #print(epi_reward)\n",
        "\n",
        "        if done:\n",
        "           s = env.reset()\n",
        "           Reward_list8[episode] = epi_reward\n",
        "           break\n",
        "\n",
        "        '''Sample minibatches from the transition'''\n",
        "        batch_s, batch_a, batch_r, batch_done, batch_s_ = agent8.memo.sample()\n",
        "\n",
        "        '''Compute Q_target'''\n",
        "        target_q_values = agent8.target_net(batch_s_)\n",
        "        target_q = batch_r + agent8.Gamma * (1-batch_done) * target_q_values.max(dim=1, keepdim=True)[0]\n",
        "        '''Compute Q_pred'''\n",
        "        pred_q_values = agent8.online_net(batch_s)\n",
        "        pred_q = torch.gather(input=pred_q_values, dim=1, index=batch_a)\n",
        "\n",
        "        '''Compute Loss, gredient descent'''\n",
        "        loss = nn.functional.smooth_l1_loss(target_q, pred_q)\n",
        "        agent8.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        agent8.optimizer.step()\n",
        "\n",
        "        '''Fix Q-target'''\n",
        "    if episode % TARGET_UPDATE ==0:\n",
        "        agent8.target_net.load_state_dict(agent8.online_net.state_dict())\n",
        "        reward = np.mean(Reward_list8[episode-10:episode])\n",
        "        print(\"Episode:{}\".format(episode))\n",
        "        print(\"Reward:{}\".format(reward))\n",
        "        episode_list8.append(episode)\n",
        "        reward_list8.append(reward)"
      ],
      "metadata": {
        "id": "3JT4QyXcJ0Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Different Lr\n",
        "plt.figure(figsize=(10, 6), dpi=300)\n",
        "plt.plot(episode_list, reward_list2, label='lr=0.001')\n",
        "plt.plot(episode_list, reward_list, label='lr=0.01')\n",
        "plt.plot(episode_list, reward_list3, label='lr=0.05')\n",
        "plt.legend()\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.title('DQN with different learning rates')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SLMARx1GQcSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Different QN layers\n",
        "plt.figure(figsize=(10, 6), dpi=300)\n",
        "plt.plot(episode_list, reward_list, label='1 layer')\n",
        "plt.plot(episode_list, reward_list4, label='2 layers')\n",
        "plt.plot(episode_list, reward_list7, label='3 layers')\n",
        "#plt.plot(episode_list, reward_list6, label='QN4')\n",
        "plt.legend(fontsize=18)\n",
        "plt.xlabel('Episode',fontsize=16)\n",
        "plt.ylabel('Reward',fontsize=16)\n",
        "plt.xticks(fontsize=16 )\n",
        "plt.yticks(fontsize=16 )\n",
        "plt.title('DQN with different Q-network layers',fontsize=24)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dmGpP27rQgEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Different number of neurons\n",
        "plt.figure(figsize=(10, 6), dpi=300)\n",
        "plt.plot(episode_list, reward_list, label='64 neurons')\n",
        "plt.plot(episode_list, reward_list5, label='32 neurons')\n",
        "plt.plot(episode_list, reward_list8, label='128 neurons')\n",
        "#plt.plot(episode_list, reward_list6, label='QN4')\n",
        "plt.legend(fontsize=18)\n",
        "plt.xlabel('Episode',fontsize=16)\n",
        "plt.ylabel('Reward',fontsize=16)\n",
        "plt.xticks(fontsize=16 )\n",
        "plt.yticks(fontsize=16 )\n",
        "plt.title('DQN with different Q-network neurons',fontsize=24)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E2xy7-QODz6f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}