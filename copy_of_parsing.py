# -*- coding: utf-8 -*-
"""Copy of parsing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PNW5DRTcQ3-thUsl8BqJPB4Qxxh5op_6
"""

import gym
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
import matplotlib.pyplot as plt
import requests
import numpy as np


class Replaybuffer:
    def __init__(self,n_state,n_action):
        self.n_state = n_state
        self.n_action = n_action
        self.size = 2000 #max capacity of replay buffer
        self.batchsize = 10

        #initialise batches
        self.s = np.empty(shape = (self.size, self.n_state), dtype=np.float32)
        self.a = np.random.randint(low=0, high=n_action, size=self.size, dtype=np.uint8)
        self.r = np.empty(self.size, dtype=np.float32)
        self.done = np.random.randint(low=0, high=2, size=self.size, dtype=np.uint8)
        self.s_ = np.empty(shape = (self.size, self.n_state), dtype=np.float32)

        self.t = 0
        self.tmax = 0  # initialise tmax

    def add_memo(self,s,a,r,done,s_): #add to replay buffer
        self.s[self.t] = s
        self.a[self.t] = a
        self.r[self.t] = r
        self.done[self.t] = done
        self.s_[self.t] = s_
        self.t = self.t + 1 if self.t + 1 < self.size else 1 #if 2001, then reset to 1
        self.tmax = max(self.tmax, self.t +1)

    def sample(self):

        if self.tmax > self.batchsize:
           k = self.batchsize  # if exceeds batchsize then take batchsize # of samples
        else:
           k = self.tmax  # else take tmax # of samples

        idxes = random.sample(range(0, self.tmax), k)

        batch_s = []
        batch_a = []
        batch_r = []
        batch_done = []
        batch_s_ = []

        for idx in idxes: #extract data
            batch_s.append(self.s[idx])
            batch_a.append(self.a[idx])
            batch_r.append(self.r[idx])
            batch_done.append(self.done[idx])
            batch_s_.append(self.s_[idx])

        #convert numpy arrays to torch tensors
        batch_s = torch.as_tensor(np.asarray(batch_s),dtype=torch.float32)
        batch_a = torch.as_tensor(np.asarray(batch_a),dtype=torch.int64).unsqueeze(-1) #Dim (2,) to (2,1)
        batch_r = torch.as_tensor(np.asarray(batch_r),dtype=torch.float32).unsqueeze(-1)
        batch_done = torch.as_tensor(np.asarray(batch_done),dtype=torch.float32).unsqueeze(-1)
        batch_s_ = torch.as_tensor(np.asarray(batch_s_),dtype=torch.float32)

        return batch_s, batch_a, batch_r, batch_done, batch_s_

class Qnetwork(nn.Module):
      def __init__(self, n_input, n_output):
          super().__init__() #Inherit from the Module superclass

          self.net = nn.Sequential(
              nn.Linear(in_features= n_input, out_features = 128),
              nn.ReLU(),
              nn.Linear(in_features= 128, out_features = n_output))

      def forward(self,x):
           return self.net(x) #forward propagation

      def act(self,obs): #In the face of s, find the maximum Q value (because the neural network outputs more than the maximum Q value) and output the corresponding action
          obs_tensor = torch.as_tensor(obs, dtype=torch.float32)
          q_value = self(obs_tensor.unsqueeze(0)) #convert it to the row vector
          max_q_idx = torch.argmax(input=q_value)
          action = max_q_idx.detach().item() #the action correspoding to the index of the highest Q-value
          return action

      def act_softmax(self,obs, temp):
          #print("obs: {}".format(obs))
          obs_tensor = torch.as_tensor(obs, dtype=torch.float32)
          obs_tensor = obs_tensor.unsqueeze(0)
          q_values = self(obs_tensor)
          q_values = q_values.squeeze()

          softmax_output = F.softmax(q_values / temp, dim=0)
          action_probs = torch.distributions.Categorical(softmax_output)
          action = action_probs.sample().item()
          return action

class AgentwoTNwER: #without target network with experience replay
   def __init__(self, n_input, n_output, Gamma=0.97, learning_rate = 0.01):
            self.n_input = n_input
            self.n_output = n_output
            self.learning_rate = learning_rate
            self.Gamma = Gamma
            self.memo = Replaybuffer(self.n_input, self.n_output)

            self.online_net = Qnetwork(self.n_input, self.n_output)

            self.optimizer = torch.optim.Adam(self.online_net.parameters(),lr=self.learning_rate)


class AgentwTNwER: #with target network, with experience replay
   def __init__(self, n_input, n_output, Gamma=0.97, learning_rate = 0.01):
            self.n_input = n_input
            self.n_output = n_output
            self.learning_rate = learning_rate
            self.Gamma = Gamma
            self.memo = Replaybuffer(self.n_input, self.n_output)

            self.online_net = Qnetwork(self.n_input, self.n_output)
            self.target_net = Qnetwork(self.n_input, self.n_output)

            self.optimizer = torch.optim.Adam(self.online_net.parameters(),lr=self.learning_rate)

class AgentwTNwoER: #with target network, without experience replay
   def __init__(self, n_input, n_output, Gamma=0.97, learning_rate = 0.01):
            self.n_input = n_input
            self.n_output = n_output
            self.learning_rate = learning_rate
            self.Gamma = Gamma

            self.online_net = Qnetwork(self.n_input, self.n_output)
            self.target_net = Qnetwork(self.n_input, self.n_output)

            self.optimizer = torch.optim.Adam(self.online_net.parameters(),lr=self.learning_rate)


class AgentwoTNwoER:#without target network without experience replay
   def __init__(self, n_input, n_output, Gamma=0.97, learning_rate = 0.01):
            self.n_input = n_input
            self.n_output = n_output
            self.learning_rate = learning_rate
            self.Gamma = Gamma

            self.online_net = Qnetwork(self.n_input, self.n_output)

            self.optimizer = torch.optim.Adam(self.online_net.parameters(),lr=self.learning_rate)

def dqn_without_er_with_tn(env, agent, eflag, sflag,  epsilon_start = 1.0, epsilon_decay = 10000, epsilon_end = 0.1):
  n_episode = 1000
  n_step = 200
  temp = 0.5
  TARGET_UPDATE = 10
  Reward_list = np.empty(shape=n_episode)
  episode_array = []
  rewards_array = []
  rewards_list = []
  s = env.reset()

  for episode in range(n_episode):
    epi_reward = 0
    for step in range(n_step):
        if eflag == 1:
          'epsilon greedy with decay of epsilon'
          epsilon = np.interp(episode * n_step + step, [0, epsilon_decay], [epsilon_start, epsilon_end])

          random_sample = random.random()
          if random_sample <= epsilon:
            a = env.action_space.sample()
          else:
            a = agent.online_net.act(s)

        elif sflag == 1:
          'Softmax policy'
          a =  a = agent.online_net.act_softmax(s, temp)

        else:
          raise KeyError("either epsilon greedy or softmax policy should be chosen")

        s_, r, done, _,_ = env.step(a)
        epi_reward += r

        # Learning process
        target_q_values = agent.target_net(torch.tensor(s_).unsqueeze(0))
        target_q = r + agent.Gamma * (1 - done) * target_q_values.max(dim=1)[0]

        pred_q_values = agent.online_net(torch.tensor(s).unsqueeze(0))
        pred_q = pred_q_values[0][a]

        loss = nn.functional.smooth_l1_loss(target_q, pred_q)
        agent.optimizer.zero_grad()
        loss.backward()
        agent.optimizer.step()

        s = s_

        if done:
            s = env.reset()
            Reward_list[episode] = epi_reward
            break

    if episode % TARGET_UPDATE == 0:
        agent.target_net.load_state_dict(agent.online_net.state_dict())
        reward = np.mean(Reward_list[episode-10:episode])
        print("Episode:{}".format(episode))
        print("Reward:{}".format(reward))
        episode_array.append(episode)
        rewards_array.append(reward)
  return episode_array, rewards_array

def dqn_without_er_without_tn(env, agent, eflag, sflag, epsilon_start = 1.0, epsilon_decay = 10000, epsilon_end = 0.1):
  episode_array = []
  rewards_array = []
  n_episode = 1000
  temp = 0.5
  Reward_list = np.empty(shape=n_episode)
  TARGET_UPDATE = 10
  n_step = 200

  s = env.reset()
  for episode in range(n_episode):
    epi_reward = 0
    for step in range(n_step):
      if eflag == 1:
          'epsilon greedy with decay of epsilon'
          epsilon = np.interp(episode * n_step + step, [0, epsilon_decay], [epsilon_start, epsilon_end])

          random_sample = random.random()
          if random_sample <= epsilon:
            a = env.action_space.sample()
          else:
            a = agent.online_net.act(s)

      elif sflag == 1:
          'Softmax policy'
          a =  a = agent.online_net.act_softmax(s, temp)

      else:
          raise KeyError("either epsilon greedy or softmax policy should be chosen")

      s_, r, done, _,_ = env.step(a)
      epi_reward += r

      # Learning process
      target_q_values = agent.online_net(torch.tensor(s_).unsqueeze(0))
      target_q = r + agent.Gamma * (1 - done) * target_q_values.max(dim=1)[0]

      pred_q_values = agent.online_net(torch.tensor(s).unsqueeze(0))
      pred_q = pred_q_values[0][a]

      loss = nn.functional.smooth_l1_loss(target_q, pred_q)
      agent.optimizer.zero_grad()
      loss.backward()
      agent.optimizer.step()

      s = s_

      if done:
          s = env.reset()
          Reward_list[episode] = epi_reward
          break

    if episode % TARGET_UPDATE == 0:
      #agent.target_net.load_state_dict(agent.online_net.state_dict())
      reward = np.mean(Reward_list[episode-10:episode])
      print("Episode:{}".format(episode))
      print("Reward:{}".format(reward))
      episode_array.append(episode)
      rewards_array.append(reward)

  return episode_array, rewards_array

def dqn_with_er_without_tn(env, agent, eflag, sflag, epsilon_start = 1.0, epsilon_decay = 10000, epsilon_end = 0.1):

  ## without Target Network but with Experience Replay
  n_episode = 1000
  n_step = 200
  temp = 0.5
  TARGET_UPDATE = 10
  episode_array = []
  rewards_array = []
  Reward_list = np.empty(shape=n_episode)
  s = env.reset()

  for episode in range(n_episode):
    epi_reward = 0
    for step in range(n_step):
        if eflag == 1:
          'epsilon greedy with decay of epsilon'
          epsilon = np.interp(episode * n_step + step, [0, epsilon_decay], [epsilon_start, epsilon_end])

          random_sample = random.random()
          if random_sample <= epsilon:
            a = env.action_space.sample()
          else:
            a = agent.online_net.act(s)

        elif sflag == 1:
          'Softmax policy'
          a =  a = agent.online_net.act_softmax(s, temp)

        'Interact with the env'
        s_, r, done, _, _= env.step(a) #get action a, next state s_,r,done, info
        agent.memo.add_memo(s, a, r, done, s_) #add to replay buffer
        s = s_ #store transition
        epi_reward += r

        if done:
           s = env.reset()
           Reward_list[episode] = epi_reward #store episode reward
           break

        '''Sample minibatches from the transition'''
        batch_s, batch_a, batch_r, batch_done, batch_s_ = agent.memo.sample()

        '''Compute Q_target'''
        target_q_values = agent.online_net(batch_s_)
        target_q = batch_r + agent.Gamma * (1-batch_done) * target_q_values.max(dim=1, keepdim=True)[0]
        '''Compute Q_pred'''
        pred_q_values = agent.online_net(batch_s) #For each state in the batch, it will give the Q value of each action
        pred_q = torch.gather(input=pred_q_values, dim=1, index=batch_a)
        #According to the action index specified in batch_a, select the corresponding action from the action Q value pred_q_values ​​of each state
        '''Compute Loss, gredient descent'''
        loss = nn.functional.smooth_l1_loss(target_q, pred_q)
        agent.optimizer.zero_grad()
        loss.backward()
        agent.optimizer.step() #Descent according to gradient

    if episode % TARGET_UPDATE ==0:
        #agent.target_net.load_state_dict(agent.online_net.state_dict())
        reward = np.mean(Reward_list[episode-10:episode])
        print("Episode:{}".format(episode))
        print("Reward:{}".format(reward))
        episode_array.append(episode)
        rewards_array.append(reward)
  return episode_array, rewards_array

def dqn_with_er_with_tn(env, agent, eflag, sflag, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay = 10000):

  ##DQN (with target network AND experience replay)
  s = env.reset()
  episode_array = []
  rewards_array = []
  TARGET_UPDATE = 10
  n_episode = 1000
  n_step = 200
  temp = 0.5
  Reward_list = np.empty(shape=n_episode)


  for episode in range(n_episode):
    epi_reward = 0

    for step in range(n_step):
        if eflag == 1:
          'epsilon greedy with decay of epsilon'
          epsilon = np.interp(episode * n_step + step, [0, epsilon_decay], [epsilon_start, epsilon_end])

          random_sample = random.random()
          if random_sample <= epsilon:
            a = env.action_space.sample()
          else:
            a = agent.online_net.act(s)

        elif sflag == 1:
          'Softmax policy'
          a =  a = agent.online_net.act_softmax(s, temp)

        else:
          raise KeyError("either epsilon greedy or softmax policy should be chosen")

        'Interact with the env'
        s_, r, done, _, _= env.step(a) #get action a, next state s_,r,done, info
        agent.memo.add_memo(s, a, r, done, s_) #add to replay buffer
        s = s_ #store transition
        epi_reward += r

        if done:
           s = env.reset()
           Reward_list[episode] = epi_reward #store episode reward
           break

        '''Sample minibatches from the transition'''
        batch_s, batch_a, batch_r, batch_done, batch_s_ = agent.memo.sample()

        '''Compute Q_target'''
        target_q_values = agent.target_net(batch_s_)
        target_q = batch_r + agent.Gamma * (1-batch_done) * target_q_values.max(dim=1, keepdim=True)[0]
        '''Compute Q_pred'''
        pred_q_values = agent.online_net(batch_s) #For each state in the batch, it will give the Q value of each action
        pred_q = torch.gather(input=pred_q_values, dim=1, index=batch_a)
        #According to the action index specified in batch_a, select the corresponding action from the action Q value pred_q_values ​​of each state
        '''Compute Loss, gredient descent'''
        loss = nn.functional.smooth_l1_loss(target_q, pred_q)
        agent.optimizer.zero_grad()
        loss.backward()
        agent.optimizer.step() #Descent according to gradient

    if episode % TARGET_UPDATE ==0:
        agent.target_net.load_state_dict(agent.online_net.state_dict())
        reward = np.mean(Reward_list[episode-10:episode])
        print("Episode:{}".format(episode))
        print("Reward:{}".format(reward))
        episode_array.append(episode)
        rewards_array.append(reward)

  return episode_array, rewards_array

import argparse
import numpy as np


def main():
  np.random.seed(42)
  epsilon_start = 1.0
  epsilon_decay = 10000
  epsilon_end = 0.1

  env = gym.make('CartPole-v1', new_step_api='True')
  n_input = env.observation_space.shape[0]
  n_output = env.action_space.n
  eflag = False
  sflag = False


  parser = argparse.ArgumentParser(description='DQN')
  parser.add_argument('-ER', action='store_true', help='disable experience replay')
  parser.add_argument('-TN', action='store_true', help='disable target network')
  parser.add_argument('-Egreedy', action='store_true', help='enable e-greedy policy')
  parser.add_argument('-Softmax', action='store_true', help='enable softmax policy')

  args = parser.parse_args(['-TN','-Softmax'])

  print("Experience replay:", args.ER)
  print("Target network:", args.TN)

  #PARSE POLICY
  if args.Egreedy and args.Softmax:
     raise KeyError("Cannot do both E-greedy and Softmax. Pick one!")

  if args.Egreedy and args.Softmax == False:
    eflag = True
    sflag = False

  if args.Egreedy == False and args.Softmax == True:
    eflag = False
    sflag = True

  if args.Egreedy == False and args.Softmax == False:
     #default is e-greedy
     eflag = True
     sflag = False


  #PARSE TYPE OF NEURAL NETWORK

  if args.ER and args.TN:
    print("Do a DQN without an experience replay or target network")
    agent = AgentwoTNwoER(n_input, n_output)
    dqn_without_er_without_tn(env, agent, eflag, sflag)

  if args.ER == True and args.TN == False:
    print("Do a DQN without an experience replay but with target network")
    agent = AgentwTNwoER(n_input, n_output)
    dqn_without_er_with_tn(env, agent, eflag, sflag)


  if args.ER == False and args.TN == True:
    print("Do a DQN with an experience replay but without a target network")
    agent = AgentwoTNwER(n_input, n_output)
    dqn_with_er_without_tn(env, agent, eflag, sflag)


  if args.ER == False and args.TN == False:
    print("Do a DQN with an experience replay and target network")
    agent = AgentwTNwER(n_input, n_output)
    dqn_with_er_with_tn(env, agent, eflag, sflag)






main()